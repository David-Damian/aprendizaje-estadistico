#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Modelos de ensamble~
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Aprendizaje Estadístico">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session ensamble :exports both :results output org :tangle ../rscripts/08-ensamble.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Modelos por ensamble.\\
*Objetivo*:  En esta sección estudiaremos una forma de incorporar varios modelos para crear un modelo predictivo mas fuerte que un modelo individual. La estrategia está basada en una técnica de remuestreo que ya hemos estudiado previamente.\\
*Lectura recomendada*: Sección 8.2 de citep:James2021. Capítulo 15 de citep:Hastie2009c. 
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
  panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#remuestreo-o-bootstrap][Remuestreo o bootstrap]]
  - [[#para-pensar][Para pensar:]]
- [[#bootstrapped-aggregation-bagging][Bootstrapped aggregation: Bagging]]
  - [[#en-problemas-de-clasificación][En problemas de clasificación]]
  - [[#error-de-generalización][Error de generalización]]
  - [[#observaciones][Observaciones]]
  - [[#bagging-regresión-y-mse][Bagging, regresión y MSE]]
  - [[#bagging-y-clasificación][Bagging y clasificación]]
    - [[#bagging-y-clasificadores][Bagging y clasificadores:]]
    - [[#bagging-y-la-sabiduría-de-las-masas][Bagging y la sabiduría de las masas:]]
  - [[#observaciones][Observaciones]]
- [[#bosques-aleatorios][Bosques aleatorios]]
  - [[#motivación][Motivación]]
  - [[#sobre-ajuste][Sobre-ajuste]]
  - [[#análisis-de-ajuste][Análisis de ajuste]]
- [[#conclusiones][Conclusiones]]
- [[#referencias][Referencias]]
:END:

* Introducción 

Anteriormente, vimos los modelos predictivos basados en ~árboles de decisión~ (regresión y clasificación). En esta sección del curso estudiaremos ~distintas estrategias~ para combinarlos para obtener un mejor modelo predictivo al costo de ~interpretabilidad~. Algunos de estos modelos continúan representando el estado del arte en competencias como [[https://www.kaggle.com/][Kaggle]] para ~datos tabulares~.


#+DOWNLOADED: screenshot @ 2022-04-13 15:27:21
#+caption: Algoritmos que tienden a quedar en los primeros lugares en las competencias de Kaggle. Tomado de [[https://www.kaggle.com/code/msjgriffiths/r-what-algorithms-are-most-successful-on-kaggle/report?scriptVersionId=0][aquí]]. 
#+attr_html: :width 400 :align center
[[file:images/20220413-152721_screenshot.png]]


* Remuestreo o /bootstrap/

Utilizar técnicas de remuestreo nos permite cuantificar la variabilidad de un estimador estadístico sin necesidad de invocar un régimen asintótico para el procedimiento. Asimismo, nos permite controlar, hasta cierto punto, la variabilidad de nuestros estimadores.

#+REVEAL: split
Por ejemplo, consideremos la situación en donde tenemos una muestra de $n$ observaciones $Z_1, \ldots, Z_n$ las cuales tienen una varianza $\sigma^2$. Es fácil demostrar que la varianza de la media $\bar Z_n$ tiene una varianza $\sigma^2/n$.

#+REVEAL: split
Esto quiere decir, que podemos promediar para reducir la varianza estimada.

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
Usualmente no tenemos acceso al proceso generador de datos (ya sea $\pi_{X,Y}$ ó $\pi_X$). ¿Qué estrategia podemos utilizar? 

** /Bootstrap/

Podemos utilizar la muestra $z_1, \ldots, z_n \overset{\mathsf{iid}}{\sim} \pi$ como un /proxy/ de la población de la cual queremos generar observaciones. En este sentido, consideramos que la función de acumulación empírica (~ECDF~, por sus siglas en inglés) es un /buen/ estimador de la función de probabilidad (ó ~CDF~ por sus siglas en inglés)
\begin{align}
\pi[X  \leq x] \approx {\hat \pi}_n[X  \leq x] = \frac1n \sum_{i = 1}^{n} I_{[z_i  \leq x]}\,.
\end{align}

#+REVEAL: split
Con este procedimiento podemos generar $B$ conjuntos de datos
\begin{align}
z_1^{(b)}, \ldots, z_n^{(b)} \sim \hat \pi_n\,, \qquad b = 1, \ldots, B\,,
\end{align}
para obtener estimadores $\hat \theta^{(b)}_n = t(z_1^{(b)}, \ldots, z_n^{(b)})$ y, a través de un promedio, obtener un estimador
\begin{align}
\bar \theta_{B,n}^{(\mathsf{bag})} = \frac1B \sum_{b= 1}^{B} \hat \theta^{(b)}_n \,,
\end{align}
con varianza que se reduce a una tasa $1/B$.

#+BEGIN_NOTES
El muestreo $z_1^{(b)}, \ldots, z_n^{(b)} \sim \hat \pi_n$ implica tomar muestras *con* reemplazo del conjunto de datos observado. Nota que las remuestras son del mismo tamaño que la muestra original. Es decir, cada remuestra $b$ tiene $n$ observaciones. Como el procedimiento es con reemplazo, esto puede ocasionar que pueda haber algunas observaciones que repitan en la remuestra.  
#+END_NOTES


** Ejemplo: Suavizadores

La estrategia de remuestreo nos puede ayudar a cuantificar la estabilidad de
ciertos estimadores. Por ejemplo, consideremos los datos que teníamos sobre el
ingreso para un conjunto de 150 observaciones. El interés es construir un
suavizador que relacione ~Edad~ con ~Ingreso~. Utilizaremos un suavizador de /splines/
con 15 grados de libertad, ver [[fig:splines-smooth]].

#+begin_src R :exports none :results none 
  library(ISLR)
  set.seed(108727)
  ## Cargamos datos
  data <- tibble(Wage) |> select(year, age, wage, education) |>
    mutate(hi.income = ifelse(wage > 250, 1, 0),
           age = as.numeric(age)) |>
    sample_frac(.05)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/splines-bootstrap.jpeg :exports results :results output graphics file
  library(ggformula)
  g1.ssplines <- data |>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
              df = 15, 
              color = 'red',
              lty = 1,
              show.legend = TRUE) + 
    sin_lineas +
    ## scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 15")
    coord_cartesian(ylim = c(0, 300))
  g1.ssplines
#+end_src
#+name: fig:splines-smooth
#+caption: Suavizador por /splines/ con 15 grados de libertad. 
#+RESULTS:
[[file:../images/splines-bootstrap.jpeg]]

#+REVEAL: split
A través de remuestreo podemos cuantificar la estabilidad de dicha estimación, ver [[fig:splines-boot]].

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-splines-bootstrap.jpeg :exports results :results output graphics file
  library("rsample")
  ajusta_boot <- function(id){
    ## Creo remuestra
    data.boot <- data |>
      slice_sample(prop = 1, replace = TRUE)
    ## Ajusto modelo 
    model <- smooth.spline(y = data.boot$wage, x = data.boot$age, df = 15)
    ## Hago predicciones y las regreso (ojo no extrapola)
    predict(model, newdata = tibble(age = seq(20, 80))) |>
      as_tibble()
  }

  boot.fit <- tibble(id = 1:100) |>
    mutate(resultados = map(id, ajusta_boot))

  g1.ssplines + 
    geom_line(data = unnest(boot.fit, resultados),
              aes(x, y, group = id),
              color = 'lightblue', alpha = .2) +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
                df = 15, 
                color = 'red',
                lty = 1,
                show.legend = TRUE)
#+end_src
#+name: fig:splines-boot
#+caption: Suavizador por /splines/ con 15 grados de libertad, réplicas con remuestreo. 
#+RESULTS:
[[file:../images/wage-splines-bootstrap.jpeg]]

#+REVEAL: split

La estabilidad también la podemos graficar por medio de intervalos de confianza. Ver [[fig:splines-boot-int]].

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-splines-bootstrap-intervals.jpeg :exports results :results output graphics file
  boot.fit <- tibble(id = 1:100) |>
    mutate(resultados = map(id, ajusta_boot))

  boot.fit |>
    unnest(resultados) |>
    group_by(x) |>
    summarise(pred.lo = quantile(y, prob = .025),
              pred    = mean(y),
              pred.hi = quantile(y, prob = .975)) |>
    ggplot(aes(x, pred)) +
    geom_ribbon(aes(ymin = pred.lo,
                    ymax = pred.hi),
                fill = "lightblue", alpha = .5) +
    geom_line(color = 'red') +
    geom_point(data = data, aes(age, wage), color = "gray") +
    sin_lineas +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 15")
#+end_src
#+name: fig:splines-boot-int
#+caption: Suavizador por /splines/ con 15 grados de libertad, réplicas con remuestreo. 
#+RESULTS:
[[file:../images/wage-splines-bootstrap-intervals.jpeg]]


* /Bootstrapped aggregation: Bagging/

En el contexto de modelado predictivo nos interesa ~estimar la relación~ que existe entre atributos $x$ y la respuesta  $y$ por medio de una $f: \mathcal{X} \mapsto \mathcal{Y}$. Dicho estimador, lo denotamos por $\hat f_n$ haciendo énfasis en que ha sido construido con una muestra de tamaño $n$ al minimizar una función de pérdida adecuada.

#+REVEAL: split
Si utilizamos /bootstrap/, para cada uno de los $B$ conjuntos de entrenamiento estimamos $\hat f_n^{(b)}$ para poder hacer predicciones por medio de
\begin{align}
\hat f^{(\mathsf{bag})}_{B,n} (x) = \frac1B \sum_{b = 1}^{B} \hat f_n^{(b)} (x) \,,
\end{align}
esto lo llamamos ~bootstrap aggregation~ o ~bagging~. 

** En problemas de clasificación

Para problemas de clasificación podemos considerar las predicciones de cada uno
de los modelos $\hat f_n^{(b)}$ y tomar la clase con ~más votos~ dentro del conjunto de
$B$ predictores.

** Error de generalización

- Usando /bootstrap/ entrenamos con cada uno de los conjuntos de datos remuestreados.
- Cada conjunto remuestreado utiliza, en promedio, $2/3$ de los datos originales.
- El conjunto no utilizado lo llamamos ~conjunto fuera de bolsa~ (/out-of-bag/, ~OOB~).
- Podemos obtener predicciones para cada observación $i = 1, \ldots, n$
  cuando se encuentra en algún conjunto ~OOB~. En promedio, tenemos $B/3$ predicciones
  para cada $i$, las cuales podemos promediar para obtener la predicción en cada $i$.
- Esto es un estimador de ~LOO-CV~ utilizando ~bagging~.

** Observaciones

- El estimador $\hat f^{(\mathsf{bag})}_{B,n}$ es un estimador Monte Carlo. ¿De qué?
- El estimador $\hat f^{(\mathsf{bag})}_{B,n} \rightarrow \hat f_n$ con $B\rightarrow \infty$ en cada uno de los puntos a evaluar $x$.
- Cuando los atributos están altamente correlacionados los árboles de decisión
  pueden presentar varianza alta.  En esta situación ~bagging~ puede suavizar la
  varianza y reducir el error de generalización.

** /Bagging/, regresión y ~MSE~

- Si estamos en tareas de regresión y medimos el error de generalización por
  medio de pérdida cuadrática obtenemos lo siguiente
  \begin{align}
  \mathbb{E}\left( y - \hat f^*(x) \right)^2 \geq \mathbb{E} \left( y - \mathbb{E} \hat f^*(x) \right)^2\,,
  \end{align}
  donde $\hat f^*$ es una estimación por medio de una remuestra y $\mathbb{E}\hat f^*$ es el valor esperado de las estimaciones de $f$ utilizando remuestras.
- Por lo tanto, /bagging/ podrá disminuir el ~MSE~.

** /Bagging/ y clasificación

- En problemas de clasificación, no tenemos descomposición aditiva de sesgo y varianza.
- El uso de ~bagging~ puede hacer de un mal clasificador, algo todavía peor. Consideremos el
  caso siguiente.

*** /Bagging/ y clasificadores:
:PROPERTIES:
:reveal_background: #00468b
:END:
Supongamos que tenemos un clasificador binario que asigna $Y = 1$ para todo $x$ con probabilidad $0.4$. ¿Cuál es el la tasa de error de clasificación de este modelo? ¿Cuál sería la tasa de error de clasificación de un consenso con este modelo?

*** /Bagging/ y la sabiduría de las masas:
:PROPERTIES:
:reveal_background: #00468b
:END:
Supongamos que tenemos una colección de clasificadores independientes donde cada
uno tiene una tasa de error de $\varepsilon < 0.5$, y sea
\begin{align}
S_1(x) = \sum_{b = 1}^{B} I[G^{(b)}(x) = 1]\,,
\end{align}
el voto por consenso de que la instancia $x$ pertenezca a la clase 1. Dado que los clasificadores
son independientes entonces
\begin{align}
S_1(x) \sim \mathsf{Binomial}(B, 1- \varepsilon)\,,
\end{align}
donde
\begin{align}
\mathbb{P}(\text{ clasificación correcta }) = \mathbb{P}(S_1 > B/2) \approx 1\,,
\end{align}
con $B$ suficientemente grande.


#+BEGIN_NOTES
El resultado anterior se conoce como ~Sabiduría de las masas~ en donde se asume que cada clasificador es un clasificador ~débil~. Con tasa de error ligeramente menor al azar. Para que el consenso de dichos clasificadores tenga buenos resultados se necesita, además, que los clasificadores sean ~independientes~. 
#+END_NOTES

** Observaciones

- Utilizar ~bagging~ en un problema de clasificación con árboles no es un 
  procedimiento que utilice árboles independientes. Por lo tanto no hay garantía
  de que el consenso mejore el error de clasificación.

* Bosques aleatorios

El modelo propuesto de Bosques aleatorios (~RF~ por sus siglas en inglés) ayuda a de-correlacionar un conjunto de árboles.
Para lograr esto seguimos utilizando remuestreo para seleccionar conjuntos de datos de entrenamiento. Al mismo tiempo, con cada conjunto de remuestras, utilizamos un conjunto de $m$ predictores al azar para entrenar. Esto es, utilizamos para cada remuestra, un subconjunto distinto de predictores para entrenar un árbol. 

#+REVEAL: split
Usualmente consideramos $m\approx \sqrt{p}$. Esto permite restringir el espacio de búsqueda y dejar de utilizar consistentemente los mismos predictores en cada remuestra.

** Motivación

Si consideramos la situación donde tenemos $B$ variables $\mathsf{iid}$ cada una con varianza $\sigma^2$  entonces el promedio tendrá varianza igual $\sigma^2/B$. Si las variables son sólo $\mathsf{id}$ con correlación positiva $\rho$ , entonces el promedio tendrá varianza igual a
\begin{align}
\rho \sigma^2  + \frac{1 - \rho}{B} \sigma^2\,.
\end{align}

#+BEGIN_NOTES
Incluso aunque tomemos un número suficiente de árboles para controlar el segundo término, el primer término no desvanece con $B \rightarrow \infty$. Es por esto que bosques aleatorios busca reducir la correlación entre árboles al permitir que se ajusten a conjuntos aleatorios (en observaciones y predictores) por medio de remuestreo.
#+END_NOTES


** Sobre-ajuste

- El consenso de votos tiende a ser robusto contra sobre-ajustar y si utilizamos una $B$  (el número de árboles) suficientemente grande estabilizamos la variabilidad del error de generalización. 
- Usualmente tenemos problemas de sobre-ajuste cuando el número de predictores es alto y el número de predictores relevantes para la predicción es pequeño.

** Análisis de ajuste

La predicción de un bosque aleatorio se realiza por medio de
\begin{align}
\hat f_{\mathsf{RF}}(x) = \frac1B \sum_{b=1}^{B} T\left(x; \Theta(\mathcal{D}_n^{(b)})\right)\,,
\end{align}
donde $T(x; \Theta)$ denota la predicción de un árbol utilizando los parámetros (variables de selección, puntos de corte) $\Theta$. La notación $\Theta(\mathcal{D}_n)$ hace énfasis en que los parámetros que gobiernan el árbol fueron escogidos utilizando el conjunto de datos $\mathcal{D}_n$. El término $\mathcal{D}_n^{(b)}$ hace énfasis en que el conjunto de entrenamiento es una remuestra del conjunto original.

#+REVEAL: split
El predictor tiende a satisfacer la siguiente igualdad (ley de los grandes números, $B \rightarrow \infty$)
\begin{align}
\hat f_{\mathsf{RF}}(x) = \mathbb{E}_{\Theta | \mathcal{D}_n} T\left(x; \Theta(\mathcal{D}_n)\right)\,, 
\end{align}
donde hacemos énfasis en que es un valor esperado condicional en los datos de entrenamiento.

#+REVEAL: split
Nos interesa evaluar el ~error estándar~ de dicho estimador. Lo cual escribimos como 
\begin{align}
\mathbb{V}\left(\hat f_{\mathsf{RF}}(x) \right) = \rho(x) \cdot \sigma^2(x)\,, 
\end{align}
donde:
- $\rho(x)$ es la correlación entre dos árboles
  \begin{align}
  \rho(x) = \mathsf{Corr}\left[ T\left(x; \Theta_i(\mathcal{D}_n)\right), T\left(x; \Theta_j(\mathcal{D}_n)\right)\right]\,.
  \end{align}
- $\sigma^2(x)$ es la varianza de cualquier árbol
  \begin{align}
  \sigma^2(x) = \mathbb{V}\left(T\left(x; \Theta(\mathcal{D}_n)\right)\right)\,.
  \end{align}

  
* Conclusiones

- Los bosques aleatorios son uno de los métodos más generales de predicción.
- Son fáciles de entrenar, usualmente ajustando dos parámetros por validación cruzada.
- Heredan ventajas de los árboles. Por ejemplo, las predicciones siempre se encuentran en el rango de las observaciones.
- Pueden ser lentos en predicción.
- Tienen capacidad de extrapolación limitada. 


* Referencias                                                         :latex:


bibliographystyle:abbrvnat
bibliography:references.bib


