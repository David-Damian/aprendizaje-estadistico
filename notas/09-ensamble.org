#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Modelos de ensamble~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/09-ensamble.pdf
:END:
#+PROPERTY: header-args:R :session ensamble :exports both :results output org :tangle ../rscripts/09-ensamble.R :mkdirp yes :dir ../ :eval never
#+EXCLUDE_TAGS: toc latex noexport

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | Modelos en ensamble.\\
*Objetivo*:  En esta sección estudiaremos una forma de incorporar varios modelos para crear un modelo predictivo mas fuerte que un modelo individual. La estrategia está basada en una técnica de remuestreo que ya hemos estudiado previamente.\\
*Lectura recomendada*: Sección 8.2 de citep:James2021. Capítulo 15 de citep:Hastie2009c. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


#+begin_src R :exports none :results none 
  library(tidymodels)
#+end_src


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#remuestreo-o-bootstrap][Remuestreo o bootstrap]]
  - [[#para-pensar][Para pensar:]]
- [[#bootstrapped-aggregation-bagging][Bootstrapped aggregation: Bagging]]
  - [[#en-problemas-de-clasificación][En problemas de clasificación]]
  - [[#error-de-generalización][Error de generalización]]
  - [[#observaciones][Observaciones]]
  - [[#bagging-regresión-y-mse][Bagging, regresión y MSE]]
  - [[#bagging-y-clasificación][Bagging y clasificación]]
    - [[#bagging-y-clasificadores][Bagging y clasificadores:]]
    - [[#bagging-y-la-sabiduría-de-las-masas][Bagging y la sabiduría de las masas:]]
  - [[#observaciones][Observaciones]]
- [[#bosques-aleatorios][Bosques aleatorios]]
  - [[#motivación][Motivación]]
  - [[#sobre-ajuste][Sobre-ajuste]]
  - [[#análisis-de-ajuste][Análisis de ajuste]]
- [[#aplicación-predicción-de-precios-ikea][Aplicación: Predicción de precios IKEA]]
- [[#conclusiones][Conclusiones]]
:END:

* Introducción 

Anteriormente, vimos los modelos predictivos basados en ~árboles de decisión~ (regresión y clasificación). En esta sección del curso estudiaremos ~distintas estrategias~ para combinarlos para obtener un mejor modelo predictivo al costo de ~interpretabilidad~. Algunos de estos modelos continúan representando el estado del arte en competencias como [[https://www.kaggle.com/][Kaggle]] para ~datos tabulares~.


#+DOWNLOADED: screenshot @ 2022-04-13 15:27:21
#+caption: Algoritmos que tienden a quedar en los primeros lugares en las competencias de Kaggle. Tomado de [[https://www.kaggle.com/code/msjgriffiths/r-what-algorithms-are-most-successful-on-kaggle/report?scriptVersionId=0][aquí]]. 
#+attr_latex: :width .65\linewidth
#+attr_html: :width 400 :align center
[[file:images/20220413-152721_screenshot.png]]


* Remuestreo o /bootstrap/

Utilizar técnicas de remuestreo nos permite cuantificar la variabilidad de un estimador estadístico sin necesidad de invocar un régimen asintótico para el procedimiento. Asimismo, nos permite controlar, hasta cierto punto, la variabilidad de nuestros estimadores.

#+REVEAL: split
Por ejemplo, consideremos la situación en donde tenemos una muestra de $n$ observaciones $Z_1, \ldots, Z_n$ las cuales tienen una varianza $\sigma^2$. Es fácil demostrar que la varianza de la media $\bar Z_n$ tiene una varianza $\sigma^2/n$.

#+REVEAL: split
Esto quiere decir, que podemos promediar para reducir la varianza estimada.

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
Usualmente no tenemos acceso al proceso generador de datos (ya sea $\mathbb{P}_{X,Y}$ ó $\mathbb{P}_X$). ¿Qué estrategia podemos utilizar? 

** /Bootstrap/

Podemos utilizar la muestra $z_1, \ldots, z_n \overset{\mathsf{iid}}{\sim} \pi$ como un /proxy/ de la población de la cual queremos generar observaciones. En este sentido, consideramos que la función de acumulación empírica (~ECDF~, por sus siglas en inglés) es un /buen/ estimador de la función de probabilidad (ó ~CDF~ por sus siglas en inglés)
\begin{align}
\pi[X  \leq x] \approx {\hat \pi}_n[X  \leq x] = \frac1n \sum_{i = 1}^{n} I_{[z_i  \leq x]}\,.
\end{align}

#+REVEAL: split
Con este procedimiento podemos generar $B$ conjuntos de datos
\begin{align}
z_1^{(b)}, \ldots, z_n^{(b)} \overset{\mathsf{iid}}{\sim} \hat \pi_n\,, \qquad b = 1, \ldots, B\,,
\end{align}
para obtener estimadores $\hat \theta^{(b)}_n = t(z_1^{(b)}, \ldots, z_n^{(b)})$ y, a través de un promedio, obtener un estimador
\begin{align}
\bar \theta_{B,n}^{(\mathsf{bag})} = \frac1B \sum_{b= 1}^{B} \hat \theta^{(b)}_n \,,
\end{align}
con varianza que se reduce a una tasa $1/B$.

#+BEGIN_NOTES
El muestreo $z_1^{(b)}, \ldots, z_n^{(b)} \overset{\mathsf{iid}}{\sim} \hat \pi_n$ implica tomar muestras *con* reemplazo del conjunto de datos observado. Nota que las remuestras son del mismo tamaño que la muestra original. Es decir, cada remuestra $b$ tiene $n$ observaciones. Como el procedimiento es con reemplazo, esto puede ocasionar que pueda haber algunas observaciones que repitan en la remuestra.  
#+END_NOTES


** Ejemplo: Suavizadores

La estrategia de remuestreo nos puede ayudar a cuantificar la estabilidad de
ciertos estimadores. Por ejemplo, consideremos los datos que teníamos sobre el
ingreso para un conjunto de 150 observaciones. El interés es construir un
suavizador que relacione ~Edad~ con ~Ingreso~. Utilizaremos un suavizador de /splines/
con 15 grados de libertad, ver [[fig:splines-smooth]].

#+begin_src R :exports none :results none 
  library(ISLR)
  set.seed(108727)
  ## Cargamos datos
  data <- tibble(Wage) |> select(year, age, wage, education) |>
    mutate(hi.income = ifelse(wage > 250, 1, 0),
           age = as.numeric(age)) |>
    sample_frac(.05)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/splines-bootstrap.jpeg :exports results :results output graphics file
  library(ggformula)
  g1.ssplines <- data |>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
              df = 15, 
              color = 'red',
              lty = 1,
              show.legend = TRUE) + 
    sin_lineas +
    ## scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 15")
    coord_cartesian(ylim = c(0, 300))
  g1.ssplines
#+end_src
#+name: fig:splines-smooth
#+caption: Suavizador por /splines/ con 15 grados de libertad. 
#+RESULTS:
[[file:../images/splines-bootstrap.jpeg]]

#+REVEAL: split
A través de remuestreo podemos cuantificar la estabilidad de dicha estimación, ver [[fig:splines-boot]].

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-splines-bootstrap.jpeg :exports results :results output graphics file
  library("rsample")
  ajusta_boot <- function(id){
    ## Creo remuestra
    data.boot <- data |>
      slice_sample(prop = 1, replace = TRUE)
    ## Ajusto modelo 
    model <- smooth.spline(y = data.boot$wage, x = data.boot$age, df = 15)
    ## Hago predicciones y las regreso (ojo no extrapola)
    predict(model, newdata = tibble(age = seq(20, 80))) |>
      as_tibble()
  }

  boot.fit <- tibble(id = 1:100) |>
    mutate(resultados = map(id, ajusta_boot))

  g1.ssplines + 
    geom_line(data = unnest(boot.fit, resultados),
              aes(x, y, group = id),
              color = 'lightblue', alpha = .2) +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
                df = 15, 
                color = 'red',
                lty = 1,
                show.legend = TRUE)
#+end_src
#+name: fig:splines-boot
#+caption: Suavizador por /splines/ con 15 grados de libertad, réplicas con remuestreo. 
#+RESULTS:
[[file:../images/wage-splines-bootstrap.jpeg]]

#+REVEAL: split

La estabilidad también la podemos graficar por medio de intervalos de confianza. Ver [[fig:splines-boot-int]].

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-splines-bootstrap-intervals.jpeg :exports results :results output graphics file
  boot.fit <- tibble(id = 1:100) |>
    mutate(resultados = map(id, ajusta_boot))

  boot.fit |>
    unnest(resultados) |>
    group_by(x) |>
    summarise(pred.lo = quantile(y, prob = .025),
              pred    = mean(y),
              pred.hi = quantile(y, prob = .975)) |>
    ggplot(aes(x, pred)) +
    geom_ribbon(aes(ymin = pred.lo,
                    ymax = pred.hi),
                fill = "lightblue", alpha = .5) +
    geom_line(color = 'red') +
    geom_point(data = data, aes(age, wage), color = "gray") +
    sin_lineas +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 15")
#+end_src
#+name: fig:splines-boot-int
#+caption: Suavizador por /splines/ con 15 grados de libertad, réplicas con remuestreo. 
#+RESULTS:
[[file:../images/wage-splines-bootstrap-intervals.jpeg]]


* /Bootstrapped aggregation: Bagging/

En el contexto de modelado predictivo nos interesa ~estimar la relación~ que existe entre atributos $x$ y la respuesta  $y$ por medio de una $f: \mathcal{X} \mapsto \mathcal{Y}$. Dicho estimador, lo denotamos por $\hat f_n$ haciendo énfasis en que ha sido construido con una muestra de tamaño $n$ al minimizar una función de pérdida adecuada.

#+REVEAL: split
Si utilizamos /bootstrap/, para cada uno de los $B$ conjuntos de entrenamiento estimamos $\hat f_n^{(b)}$ para poder hacer predicciones por medio de
\begin{align}
\hat f^{(\mathsf{bag})}_{B,n} (x) = \frac1B \sum_{b = 1}^{B} \hat f_n^{(b)} (x) \,,
\end{align}
esto lo llamamos ~bootstrap aggregation~ o ~bagging~. 

** En problemas de clasificación

Para problemas de clasificación podemos considerar las predicciones de cada uno
de los modelos $\hat f_n^{(b)}$ y tomar la clase con ~más votos~ dentro del conjunto de
$B$ predictores.

** Error de generalización

- Usando /bootstrap/ entrenamos con cada uno de los conjuntos de datos remuestreados.
- Cada conjunto remuestreado utiliza, en promedio, $2/3$ de los datos originales.
- El conjunto no utilizado lo llamamos ~conjunto fuera de bolsa~ (/out-of-bag/, ~OOB~).
- Podemos obtener predicciones para cada observación $i = 1, \ldots, n$
  cuando se encuentra en algún conjunto ~OOB~. En promedio, tenemos $B/3$ predicciones
  para cada observación, las cuales podemos promediar para obtener la predicción final.
- Esto es un estimador de ~LOO-CV~ utilizando ~bagging~.

** Observaciones

- El estimador $\hat f^{(\mathsf{bag})}_{B,n}$ es un estimador Monte Carlo. ¿De qué?
- El estimador $\hat f^{(\mathsf{bag})}_{B,n} \rightarrow \hat f_n$ con $B\rightarrow \infty$ en cada uno de los puntos a evaluar $x$.
- Cuando los atributos están altamente correlacionados los árboles de decisión
  pueden presentar varianza alta.  En esta situación ~bagging~ puede suavizar la
  varianza y reducir el error de generalización.

** /Bagging/, regresión y ~MSE~

- Si estamos en tareas de regresión y medimos el error de generalización por
  medio de pérdida cuadrática obtenemos lo siguiente
  \begin{align}
  \mathbb{E}\left( y - \hat f^*(x) \right)^2 \geq \mathbb{E} \left( y - \mathbb{E} \hat f^*(x) \right)^2\,,
  \end{align}
  donde $\hat f^*$ es una estimación por medio de una remuestra y $\mathbb{E}\hat f^*$ es el valor esperado de las estimaciones de $f$ utilizando remuestras.
- Por lo tanto, /bagging/ podrá disminuir el ~MSE~.

** /Bagging/ y clasificación

- En problemas de clasificación, no tenemos descomposición aditiva de sesgo y varianza.
- El uso de ~bagging~ puede hacer de un mal clasificador, algo todavía peor. Consideremos el
  caso siguiente.

*** /Bagging/ y clasificadores:
:PROPERTIES:
:reveal_background: #00468b
:END:
Supongamos que tenemos un clasificador binario que asigna $Y = 1$ para todo $x$ con probabilidad $0.4$. ¿Cuál es el la tasa de error de clasificación de este modelo? ¿Cuál sería la tasa de error de clasificación de un consenso con este modelo?

*** /Bagging/ y la sabiduría de las masas:
:PROPERTIES:
:reveal_background: #00468b
:END:
Supongamos que tenemos una colección de clasificadores independientes donde cada
uno tiene una tasa de error de $\varepsilon < 0.5$, y sea
\begin{align}
S_1(x) = \sum_{b = 1}^{B} I[G^{(b)}(x) = 1]\,,
\end{align}
el voto por consenso de que la instancia $x$ pertenezca a la clase 1. Dado que los clasificadores
son independientes entonces
\begin{align}
S_1(x) \sim \mathsf{Binomial}(B, 1- \varepsilon)\,,
\end{align}
donde
\begin{align}
\mathbb{P}(\text{ clasificación correcta }) = \mathbb{P}(S_1 > B/2) \approx 1\,,
\end{align}
con $B$ suficientemente grande.


#+BEGIN_NOTES
El resultado anterior se conoce como ~Sabiduría de las masas~ en donde se asume que cada clasificador es un clasificador ~débil~. Con tasa de error ligeramente menor al azar. Para que el consenso de dichos clasificadores tenga buenos resultados se necesita, además, que los clasificadores sean ~independientes~. 
#+END_NOTES

** Observaciones

- Utilizar ~bagging~ en un problema de clasificación con árboles no es un 
  procedimiento que utilice árboles independientes. Por lo tanto no hay garantía
  de que el consenso mejore el error de clasificación.

* Bosques aleatorios

El modelo propuesto de Bosques aleatorios (~RF~ por sus siglas en inglés) ayuda a de-correlacionar un conjunto de árboles.
Para lograr esto seguimos utilizando remuestreo para seleccionar conjuntos de datos de entrenamiento. Al mismo tiempo, con cada conjunto de remuestras, utilizamos un conjunto de $m$ predictores al azar para entrenar. Esto es, utilizamos para cada remuestra, un subconjunto distinto de predictores para entrenar un árbol. 

#+REVEAL: split
Usualmente consideramos $m\approx \sqrt{p}$. Esto permite restringir el espacio de búsqueda y dejar de utilizar consistentemente los mismos predictores en cada remuestra.

** Motivación

Si consideramos la situación donde tenemos $B$ variables $\mathsf{iid}$ cada una con varianza $\sigma^2$  entonces el promedio tendrá varianza igual $\sigma^2/B$. Si las variables son sólo $\mathsf{id}$ con correlación positiva $\rho$ , entonces el promedio tendrá varianza igual a
\begin{align}
\rho \sigma^2  + \frac{1 - \rho}{B} \sigma^2\,.
\end{align}

#+BEGIN_NOTES
Incluso aunque tomemos un número suficiente de árboles para controlar el segundo término, el primer término no desvanece con $B \rightarrow \infty$. Es por esto que bosques aleatorios busca reducir la correlación entre árboles al permitir que se ajusten a conjuntos aleatorios (en observaciones y predictores) por medio de remuestreo.
#+END_NOTES


** Sobre-ajuste

- El consenso de votos tiende a ser robusto contra sobre-ajustar y si utilizamos una $B$  (el número de árboles) suficientemente grande estabilizamos la variabilidad del error de generalización. 
- Usualmente tenemos problemas de sobre-ajuste cuando el número de predictores es alto y el número de predictores relevantes para la predicción es pequeño.

** Análisis de ajuste

La predicción de un bosque aleatorio se realiza por medio de
\begin{align}
\hat f_{\mathsf{RF}}(x) = \frac1B \sum_{b=1}^{B} \hat f^{(b)}(x) = \frac1B \sum_{b=1}^{B} T\left(x; \Theta(\mathcal{D}_n^{(b)})\right)\,,
\end{align}
donde $T(x; \Theta)$ denota la predicción de un árbol utilizando los parámetros (variables de selección, puntos de corte) $\Theta$. La notación $\Theta(\mathcal{D}_n)$ hace énfasis en que los parámetros que gobiernan el árbol fueron escogidos utilizando el conjunto de datos $\mathcal{D}_n$. El término $\mathcal{D}_n^{(b)}$ hace énfasis en que el conjunto de entrenamiento es una remuestra del conjunto original.

#+REVEAL: split
El predictor tiende a satisfacer la siguiente igualdad (ley de los grandes números, $B \rightarrow \infty$)
\begin{align}
\hat f_{\mathsf{RF}}(x) = \mathbb{E}_{\Theta | \mathcal{D}_n} T\left(x; \Theta(\mathcal{D}_n)\right)\,, 
\end{align}
donde hacemos énfasis en que es un valor esperado condicional en los datos de entrenamiento.

#+REVEAL: split
Nos interesa evaluar el ~error estándar~ de dicho estimador. Lo cual escribimos como 
\begin{align}
\mathsf{SE}\left(\hat f_{\mathsf{RF}}(x)  \right)^2 = \mathbb{V}\left(\hat f_{\mathsf{RF}}(x) \right) = \rho(x) \cdot \sigma^2(x)\,, 
\end{align}
donde:
- $\rho(x)$ es la correlación entre dos árboles
  \begin{align}
  \rho(x) = \mathsf{Corr}\left[ T\left(x; \Theta_i(\mathcal{D}_n)\right), T\left(x; \Theta_j(\mathcal{D}_n)\right)\right]\,.
  \end{align}
- $\sigma^2(x)$ es la varianza de cualquier árbol
  \begin{align}
  \sigma^2(x) = \mathbb{V}\left(T\left(x; \Theta(\mathcal{D}_n)\right)\right)\,.
  \end{align}


* Aplicación: Predicción de árboles con bosques aleatorios         :noexport:

#+begin_src R :exports none :results none  :tangle no
  ## Aplicación: Arboles en San Francisco --------------------------------------
  sf_trees <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-28/sf_trees.csv", show_col_types = FALSE, progress = FALSE)
#+end_src


#+begin_src R :exports none :results none :tangle no
  trees_df <- sf_trees |>
    mutate(
      legal_status = case_when(
        legal_status == "DPW Maintained" ~ legal_status,
        TRUE ~ "Other"
      ),
      plot_size = parse_number(plot_size)
    ) |>
    select(-address)|>
    na.omit() |>
    mutate_if(is.character, factor)
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-localizacion.jpeg :exports results :results output graphics file :tangle no
  trees_df |>
    ggplot(aes(longitude, latitude, color = legal_status)) +
    geom_point(size = 0.5, alpha = 0.4) +
    labs(color = NULL) +
  sin_lineas + coord_equal()
#+end_src

#+RESULTS:
[[file:../images/arboles-localizacion.jpeg]]

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-cuidadores.jpeg :exports results :results output graphics file :tangle no
trees_df |>
  count(legal_status, caretaker) |>
  add_count(caretaker, wt = n, name = "caretaker_count") |>
  filter(caretaker_count > 50) |>
  group_by(legal_status) |>
  mutate(percent_legal = n / sum(n)) |>
  ggplot(aes(percent_legal, caretaker, fill = legal_status)) +
  geom_col(position = "dodge") +
  labs(
    fill = NULL,
    x = "% of trees in each category"
  ) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/arboles-cuidadores.jpeg]]

#+begin_src R :exports none :results none :tangle no
  set.seed(108727)
  trees_split <- initial_split(trees_df, strata = legal_status, prop = 1/2)
  trees_train <- training(trees_split)
  trees_test <- testing(trees_split)
#+end_src

#+begin_src R :exports none :results none :tangle no
  tree_rec <- recipe(legal_status ~ ., data = trees_train) |>
    update_role(tree_id, new_role = "ID") |>
    step_other(species, caretaker, threshold = 0.01) |>
    step_other(site_info, threshold = 0.005) |>
    step_dummy(all_nominal(), -all_outcomes()) |>
    step_date(date, features = c("year")) |>
    step_rm(date)
#+end_src

#+begin_src R :exports none :results none :tangle no
  tune_spec <- rand_forest(
    mtry = tune(),
    trees = 1000,
    min_n = tune()
  ) |>
    set_mode("classification") |>
    set_engine("ranger", importance = "permutation")
#+end_src

#+begin_src R :exports none :results none :tangle no
  tune_wf <- workflow() |>
    add_recipe(tree_rec) |>
    add_model(tune_spec)
#+end_src

#+begin_src R :exports none :results none :tangle no
  set.seed(108727)
  trees_folds <- vfold_cv(trees_train, 10)
#+end_src

#+begin_src R :exports code :results org 
  tree_grid <- grid_random(mtry(c(1,35)),
                           min_n(),
                           size = 20)
  tree_grid |> print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 20 × 2
   mtry min_n
  <int> <int>
1    29     2
2    25     6
3     9    27
4    16    10
5    26    22
# … with 15 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+begin_src R :exports none :results none :tangle no
  doParallel::registerDoParallel()

  set.seed(108727)
  tune_res <- tune_grid(
    tune_wf,
    grid = tree_grid, 
    resamples = trees_folds,
    control = control_grid(parallel_over = "resamples", verbose = TRUE)
  )
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-resultados-vc-bosques.jpeg :exports results :results output graphics file :tangle no
  tune_res |>
    collect_metrics() |>
    filter(.metric == "roc_auc") |>
    select(mean, min_n, mtry) |>
    pivot_longer(min_n:mtry,
                 values_to = "value",
                 names_to = "parameter"
                 ) |>
    ggplot(aes(value, mean, color = parameter)) +
    geom_point(show.legend = FALSE) +
    facet_wrap(~parameter, scales = "free_x") +
    labs(x = NULL, y = "AUC") + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/arboles-resultados-vc-bosques.jpeg]]

#+begin_src R :exports code :results none :tangle no
  rf_grid <- grid_regular(
    mtry(range = c(10, 30)),
    min_n(range = c(2, 8)),
    levels = 5
  )
#+end_src

#+begin_src R :exports code :results none  :tangle no
  set.seed(108727)
  regular_res <- tune_grid(
    tune_wf,
    resamples = trees_folds,
    grid = rf_grid,
    control = control_grid(parallel_over = "resamples")
  )
#+end_src

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-vc-zoom-bosques.jpeg :exports results :results output graphics file
  regular_res |>
    collect_metrics() |>
    filter(.metric == "roc_auc") |>
    mutate(min_n = factor(min_n)) |>
    ggplot(aes(mtry, mean, color = min_n)) +
    geom_line(alpha = 0.5, linewidth = 1.5) +
    geom_point() +
    labs(y = "AUC") + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/arboles-vc-zoom-bosques.jpeg]]

#+begin_src R :exports both :results org :tangle no
  best_auc <- select_best(regular_res, "roc_auc")

  final_rf <- finalize_model(
    tune_spec,
    best_auc
  )

  final_rf
#+end_src

#+RESULTS:
#+begin_src org
Random Forest Model Specification (classification)

Main Arguments:
  mtry = 25
  trees = 1000
  min_n = 2

Engine-Specific Arguments:
  importance = permutation

Computational engine: ranger
#+end_src

#+begin_src R :exports code :results org  :tangle no

  final_wf <- workflow() |>
    add_recipe(tree_rec) |>
    add_model(final_rf)

  final_res <- final_wf |>
    last_fit(trees_split)

  final_res |>
    collect_metrics()

#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 2 × 4
  .metric  .estimator .estimate .config             
  <chr>    <chr>          <dbl> <chr>               
1 accuracy binary         0.861 Preprocessor1_Model1
2 roc_auc  binary         0.919 Preprocessor1_Model1
#+end_src


#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-bosque-importancia.jpeg :exports results :results output graphics file
  library(vip)

  final_res |>
    extract_fit_engine() |>
    vip() + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/arboles-bosque-importancia.jpeg]]

#+begin_src R :exports results :results org :tangle no
  final_res |>
    collect_predictions() |>
    conf_mat(legal_status, .pred_class)
#+end_src

#+RESULTS:
#+begin_src org
                Truth
Prediction       DPW Maintained Other
  DPW Maintained          14819  1874
  Other                    1017  3152
#+end_src

#+begin_src R :exports results :results org :tangle no
  final_res |>
    collect_predictions() |>
    recall(legal_status, .pred_class)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 1 × 3
  .metric .estimator .estimate
  <chr>   <chr>          <dbl>
1 recall  binary         0.936
#+end_src



* Aplicación: Predicción de precios IKEA

#+begin_src R :exports none :results none
  ## Aplicacion: Precios de IKEA ---------------------------------------------
  ikea <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv")
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/ikea-precios-exploratorio.jpeg  :exports results :results output graphics file
  ikea |>
    select(`...1`, price, depth:width) |>
    pivot_longer(depth:width, names_to = "dim") |>
    ggplot(aes(value, price, color = dim)) +
    geom_point(alpha = 0.4, show.legend = FALSE) +
    scale_y_log10() +
    facet_wrap(~dim, scales = "free_x") +
    labs(x = NULL) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/ikea-precios-exploratorio.jpeg]]

#+begin_src R :exports both :results org 
  ikea_df <- ikea |>
    select(price, name, category, depth, height, width) |>
    mutate(price = log10(price)) |>
    mutate_if(is.character, factor)

  ikea_df |> print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 3,694 × 6
  price name                  category      depth height width
  <dbl> <fct>                 <fct>         <dbl>  <dbl> <dbl>
1  2.42 FREKVENS              Bar furniture    NA     99    51
2  3.00 NORDVIKEN             Bar furniture    NA    105    80
3  3.32 NORDVIKEN / NORDVIKEN Bar furniture    NA     NA    NA
4  1.84 STIG                  Bar furniture    50    100    60
5  2.35 NORBERG               Bar furniture    60     43    74
# … with 3,689 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+begin_src R :exports code :results none 
  set.seed(123)
  ikea_split <- initial_split(ikea_df, strata = price)
  ikea_train <- training(ikea_split)
  ikea_test <- testing(ikea_split)

  set.seed(234)
  ikea_folds <- vfold_cv(ikea_train, strata = price)

#+end_src


#+begin_src R :exports code :results none 
  library(textrecipes)
  ranger_recipe <-
    recipe(formula = price ~ ., data = ikea_train) |>
    step_other(name, category, threshold = 0.01) |>
    step_clean_levels(name, category) |>
    step_impute_knn(depth, height, width)

  ranger_spec <-
    rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |>
    set_mode("regression") |>
    set_engine("ranger")

  ranger_workflow <-
    workflow() |>
    add_recipe(ranger_recipe) |>
    add_model(ranger_spec)

  set.seed(8577)
  ## Create a cluster object and then register: 
  cl <- makePSOCKcluster(6)
  doParallel::registerDoParallel(cl)

  ranger_tune <-
    tune_grid(ranger_workflow,
              resamples = ikea_folds,
              grid = 11,
              control = control_grid(parallel_over = "resamples", verbose = TRUE)              
              )
#+end_src

#+begin_src R :exports both :results org 
  show_best(ranger_tune, metric = "rmse")
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 5 × 8
   mtry min_n .metric .estimator  mean     n std_err .config              
  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
1     2     4 rmse    standard   0.323    10 0.00602 Preprocessor1_Model10
2     5     6 rmse    standard   0.331    10 0.00562 Preprocessor1_Model06
3     4    10 rmse    standard   0.332    10 0.00570 Preprocessor1_Model05
4     3    18 rmse    standard   0.339    10 0.00569 Preprocessor1_Model01
5     2    21 rmse    standard   0.343    10 0.00561 Preprocessor1_Model08
#+end_src

#+begin_src R :exports both :results org 
  show_best(ranger_tune, metric = "rsq")
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 5 × 8
   mtry min_n .metric .estimator  mean     n std_err .config              
  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
1     2     4 rsq     standard   0.752    10  0.0106 Preprocessor1_Model10
2     5     6 rsq     standard   0.740    10  0.0100 Preprocessor1_Model06
3     4    10 rsq     standard   0.738    10  0.0101 Preprocessor1_Model05
4     3    18 rsq     standard   0.728    10  0.0104 Preprocessor1_Model01
5     2    21 rsq     standard   0.723    10  0.0107 Preprocessor1_Model08
#+end_src

#+begin_src R :exports both :results org 
  final_rf <- ranger_workflow |>
    finalize_workflow(select_best(ranger_tune))

  final_rf
#+end_src

#+RESULTS:
#+begin_src org
Warning message:
No value of `metric` was given; metric 'rmse' will be used.
== Workflow ==================================================================
Preprocessor: Recipe
Model: rand_forest()

-- Preprocessor --------------------------------------------------------------
3 Recipe Steps

• step_other()
• step_clean_levels()
• step_impute_knn()

-- Model ---------------------------------------------------------------------
Random Forest Model Specification (regression)

Main Arguments:
  mtry = 2
  trees = 1000
  min_n = 4

Computational engine: ranger
#+end_src

#+begin_src R :exports both :results org 
  ikea_fit <- last_fit(final_rf, ikea_split)
  ikea_fit
#+end_src

#+RESULTS:
#+begin_src org
# Resampling results
# Manual resampling 
# A tibble: 1 × 6
  splits             id               .metrics .notes   .predictions .workflow 
  <list>             <chr>            <list>   <list>   <list>       <list>    
1 <split [2770/924]> train/test split <tibble> <tibble> <tibble>     <workflow>
#+end_src

#+begin_src R :exports both :results org 
  collect_metrics(ikea_fit)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 2 × 4
  .metric .estimator .estimate .config             
  <chr>   <chr>          <dbl> <chr>               
1 rmse    standard       0.318 Preprocessor1_Model1
2 rsq     standard       0.752 Preprocessor1_Model1
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/ikea-predicciones-arboles.jpeg :exports results :results output graphics file
  collect_predictions(ikea_fit) |>
    ggplot(aes(price, .pred)) +
    geom_abline(lty = 2, color = "gray50") +
    geom_point(alpha = 0.5, color = "midnightblue") +
    coord_fixed() + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/ikea-predicciones-arboles.jpeg]]

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/ikea-vup-bosque.jpeg :exports results :results output graphics file
  library(vip)

  imp_spec <- ranger_spec |>
    finalize_model(select_best(ranger_tune)) |>
    set_engine("ranger", importance = "permutation")

  workflow() |>
    add_recipe(ranger_recipe) |>
    add_model(imp_spec) |>
    fit(ikea_train) |>
    pull_workflow_fit() |>
    vip(aesthetics = list(alpha = 0.8, fill = "midnightblue")) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/ikea-vup-bosque.jpeg]]

* Conclusiones

- Los bosques aleatorios son uno de los métodos más generales de predicción.
- Son fáciles de entrenar, usualmente ajustando dos parámetros por validación cruzada.
- Heredan ventajas de los árboles. Por ejemplo, las predicciones siempre se encuentran en el rango de las observaciones.
- Pueden ser lentos en predicción.
- Tienen capacidad de extrapolación limitada. 


bibliographystyle:abbrvnat
bibliography:references.bib


