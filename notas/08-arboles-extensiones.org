 #+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Extensiones de árboles~
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/08-arboles-extensiones.pdf
:END:
#+STARTUP: showall
#+PROPERTY: header-args:R :session arboles-ext :exports both :results output org :tangle ../rscripts/08-arboles-extensiones.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | Extensiones Arboles.\\
*Objetivo*: Que veremos.\\
*Lectura recomendada*: Referencia.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Table of Contents                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#para-pensar][Para pensar:]]
- [[#ejemplo-de-partykit][Ejemplo de partykit]]
- [[#conclusiones][Conclusiones]]
:END:

* Introducción

Los árboles de decisión por construcción son insensibles a:
- Variables continuas con /outliers/.
- Variables continuas con diferentes escalas.
- Variables categóricas (no se necesitan transformar a /dummies/).

El algoritmo de ajuste de un árbol de decisión, en regresión por ejemplo, busca
minimizar la función de pérdida
\begin{align}
R_\alpha(T) = \sum_{m = 1}^{|T|} \sum_{i: x_i \in R_m}^{} (y_i - \hat{y}_{R_m})^2 + \alpha |T|\,,
\end{align}
donde $\hat{y}_{R_m} = \sum_{i : x_i \in R_m} y_i/n_m$ es el promedio de las
respuestas en la región $m\text{-ésima}$, a tráves de ir buscando
secuencialmente las regiones $R_m$ que logren la máxima disminución de
$\mathsf{RSS}$.

La búsqueda se puede lograr por medio de un algoritmo voraz (/greedy/) que escoge
variables y puntos de corte.

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
En nuestra última discusión sobre árboles de decisión es que tienen un sesgo al
utilizar variables categóricas con muchas etiquetas. ¿Por qué?

** Motivación

Consideremos el experimento siguiente. Tenemos una colección de 7 atributos y
una respuesta.  Los datos son generados de manera aleatoria.

#+begin_src R :exports none :results none
  nexp <- 5000; nsamples <- 100;
  generate_data <- function(nsamples = 100){
    tibble(id = 1:nsamples) |>
      mutate(  y = map_dbl(id, ~rnorm(1)),
             ch2 = map_dbl(id, ~rchisq(1, 2)),
             m2  = factor(map_dbl(id, ~sample(1:2, 1))),
             m4  = factor(map_dbl(id, ~sample(1:4, 1))),
             m10 = factor(map_dbl(id, ~sample(1:10, 1))),
             m20 = factor(map_dbl(id, ~sample(1:20, 1))),
             nor = map_dbl(id, ~rnorm(1)),
             uni = map_dbl(id, ~runif(1))) |>
      select(-id)
  }
#+end_src

#+begin_src R :exports code :results org 
  set.seed(108727)
  generate_data(nsamples = 100) |>
      print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 100 × 8
       y    ch2 m2    m4    m10   m20      nor     uni
   <dbl>  <dbl> <fct> <fct> <fct> <fct>  <dbl>   <dbl>
1  2.08  1.86   1     2     10    16     0.107 0.00758
2  0.804 2.40   2     3     9     4     -0.309 0.804  
3  0.313 1.29   2     2     4     20     1.18  0.658  
4 -0.617 5.24   1     3     10    4     -0.615 0.890  
5  0.516 0.0947 2     2     10    2      1.52  0.740  
# … with 95 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+begin_src R :exports none :results none
  get_root <- function(tree_fit){
    tree_rpart_obj <- extract_fit_engine(tree_fit)
    tree_rpart_obj$frame$var[1]
  }
#+end_src

#+REVEAL: split
De manera artificial consideraremos que la respuesta $y$ está relacionada con
los demás atributos (sabemos que no es cierto).

#+begin_src R :exports code :results none
  fit_tree <- function(engine){
    data_train <- generate_data() 

    tree_spec <- decision_tree(tree_depth = 2) |>
      set_engine(engine) |>
      set_mode("regression")

    tree_spec |>
      fit(y ~ ., data = data_train) |>
      extract_fit_engine() |>
      vi() |>
      filter(Importance > 0) |> 
      mutate(rank = min_rank(desc(Importance)))
  }
#+end_src

#+begin_src R :exports none :results none :eval never
  library(tidymodels)
  library(bonsai)
  library(vip)
  nexp <- 5000
  results <- tibble(id = 1:(2*nexp)) |>
    mutate(engine = rep(c("rpart", "partykit"), each = nexp)) |>
    mutate(model  = map(engine, fit_tree))
#+end_src


\newpage
#+REVEAL: split
Repetimos este proceso (generar datos aleatorios y ajustar un árbol) un número
determinado de veces y registramos cuántas veces cada atributo fue utilizado en
el nodo raíz.

En contraste, con un modelo denominado ~conditional trees~ somos capaces de evitar
ese sesgo. Incluso podemos evitar escoger variables de corte que no tienen
asociación con la respuesta.


#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/biased-recursive-partitioning.jpeg :exports results :results output graphics file :eval never
  results |>
    unnest(model) |>
    filter(rank == 1) |>
    group_by(engine, Variable) |>
    summarise(prop = sum(rank)/nexp, .groups = "drop") |>
    mutate(engine = factor(engine, levels = c("rpart", "partykit"))) |>
    ggplot(aes(Variable)) +
    geom_bar(aes(y = prop), stat = "identity") + sin_lineas +
    geom_hline(yintercept = 1/7, lty = 2) +
    geom_hline(yintercept = 0.05/7, lty = 2, color = 'salmon') +
    facet_wrap(~engine) + 
    ylab("Proporción como nodo raíz") + xlab("")
#+end_src
#+caption: La línea negra representa la probabilidad de seleccionar una variable al azar como nodo raíz ($p = 1/7$) y la cruva salmón representa la tolerancia de error (tasa de falsos positivos) ajustada ($\alpha = 0.05/7$). 
#+RESULTS:
[[file:../images/biased-recursive-partitioning.jpeg]]

** Ejemplos de pruebas de hipótesis de independencia

* Ejemplo de partykit

* Conclusiones

El modelo de ~CTree~ es un modelo:
1. que utiliza pruebas de hipótesis para determinar variables y puntos de corte;
2. tiene un mecanismo de selección insesgado;
3. no requiere mucho post-procesamiento (poda);


#+REVEAL: split
De acuerdo a citet:Greenwell2022 , aún cuando ~CTree~ tiene mejores propiedades
estadística que ~CART~ hay un uso generalizado por el último debido a herramientas
de código abierto.

#+REVEAL: split
Además, de acuerdo a citet:Loh2014 mientras un árbol se escoja por cuestiones
predictivas y no por inferencia tiene un riesgo bajo al utilizar procedimientos
con sesgo.  Por otro lado, validación cruzada puede ayudar a eliminar ramas
redundantes durante el proceso de poda (mientras tengamos pocos atributos y un
número suficiente de datos).


bibliographystyle:abbrvnat
bibliography:references.bib
