#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Aprendizaje no Supervisado~
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Aprendizaje Estadistico">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+EXCLUDE_TAGS: toc latex
#+PROPERTY: header-args:R :session unsupervised :exports both :results output org :tangle ../rscripts/12-aprendizaje-nosupervisado.R :mkdirp yes :dir ../

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Aprendizaje no supervisado.\\
*Objetivo*: En esta última sección del curso cambiaremos el enfoque hacia aprendizaje no supervisado con el objetivo de estudiar las técnicas mas usuales: reducción de dimensiones y agrupación de datos. En particular veremos algunas técnicas que se pueden usar en este contexto.\\
*Lectura recomendada*: Capítulo 12 de citep:James2021. 
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#objetivos-de-aprendizaje-no-supervisado][Objetivos de aprendizaje no supervisado]]
  - [[#retos-del-aprendizaje-no-supervisado][Retos del aprendizaje no supervisado]]
  - [[#ventajas][Ventajas]]
- [[#métodos-de-reducción-de-dimensiones][Métodos de reducción de dimensiones]]
  - [[#observaciones][Observaciones]]
- [[#análisis-de-componentes-principales][Análisis de componentes principales]]
  - [[#el-modelo][El modelo]]
  - [[#proceso][Proceso]]
  - [[#formulación][Formulación]]
  - [[#geometría][Geometría]]
  - [[#proceso-más-componentes][Proceso: más componentes]]
  - [[#solución][Solución]]
  - [[#ejemplo][Ejemplo:]]
  - [[#interpretación][Interpretación]]
  - [[#escala-en-los-atributos][Escala en los atributos]]
  - [[#proporción-de-varianza-explicada][Proporción de varianza explicada]]
  - [[#cuántos-componentes-habrá-que-utilizar][¿Cuántos componentes habrá que utilizar?]]
- [[#segmentación][Segmentación]]
  - [[#conglomerados-para-segmentación-de-mercados][Conglomerados para segmentación de mercados]]
  - [[#métodos-de-segmentación-ejemplos][Métodos de segmentación (ejemplos)]]
- [[#k-medias][$K$ medias]]
  - [[#cómo-funciona][¿Cómo funciona?]]
  - [[#algoritmo][Algoritmo]]
  - [[#propiedades-del-algoritmo][Propiedades del algoritmo]]
- [[#agrupación-jerárquica][Agrupación jerárquica]]
  - [[#puntos-a-considerar][Puntos a considerar]]
- [[#referencias][Referencias]]
:END:


* Introducción

- Hasta ahora nos hemos concentrado en ~aprendizaje supervisado~.
- Esto es, consideramos que tenemos una colección observaciones $\{(x_i, y_i)\}_{i = 1}^n$.
- El objetivo es encontrar
  \begin{align}
  y = f(x) + \epsilon\,.
  \end{align}
- Ahora, estudiaremos técnicas de ~aprendizaje no supervisado~. Sólo tendremos
  atributos (características) $x_i \in \mathbb{R}^p$.


** Objetivos de aprendizaje no supervisado

- Descubrir cosas interesantes acerca de nuestras observaciones.
- La discusión se concentrará en:
  1. Métodos de ~reducción de dimensiones~ (visualización, o pre-procesamiento). 
  2. Métodos de ~agrupación~ (visualización o pre-procesamiento).


** Retos del aprendizaje no supervisado

- Es un mecanismo mucho mas ~subjetivo~ (mayor comunicación con expertos en el área).
- Identificar patrones cuando no es claro el qué se está buscando.

** Ventajas

- Es mucho mas sencillo obtener ~datos sin etiquetas~ que obtener datos con un objetivo claro.
- Aplicaciones mucho mas abiertas:
  - Encontrar grupos de pacientes con alguna característica en común.
  - Segmentación de clientes de acuerdo a patrones de compra.
  - Segmentación de creadores de contenido en redes sociales.


* Métodos de reducción de dimensiones

- Buscan representaciones de menor dimensionalidad. Usualmente con el objetivo de crear visualizaciones.
  
#+DOWNLOADED: screenshot @ 2022-05-18 10:57:06
#+caption: Los métodos de reducción de dimensiones pueden ser lineales (izquierda) o no-lineales (derecha). 
#+attr_html: :width 1200 :align center
  [[file:images/20220518-105706_screenshot.png]]


#+REVEAL: split
- Factorización de matrices: sistemas de recomendación o modelado de tópicos. 
#+DOWNLOADED: screenshot @ 2022-05-17 19:19:34
#+caption: Modelo de tópicos sobre documentos en términos de contenido. Imagen tomada del curso de /Probabilistic ML/ de Phillip Hennig.
#+attr_html: :width 1200 :align center
[[file:images/20220517-191934_screenshot.png]]


#+REVEAL: split
- Aprendizaje de variedades: los datos pueden tener una estructura de subespacio.
#+DOWNLOADED: screenshot @ 2022-05-17 19:36:33
#+caption: Imagen tomada de la escuela de verano en procesos Gaussianos 2014. Universidad de Sheffield. 
#+attr_latex: :width .65\linewidth
#+attr_html: :width 900 :align center
[[file:images/20220517-193633_screenshot.png]]

  
#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-17 19:38:57
#+caption: Imagen tomada de citep:Prisacariu2012. Modelo de seguimiento de poses. 
#+attr_html: :width 1200 :align center
[[file:images/20220517-193857_screenshot.png]]

#+REVEAL: split
- Arquitecturas de auto-codificación: Modelos basados en redes neuronales. 

#+DOWNLOADED: screenshot @ 2022-05-18 11:01:43
#+caption: Se pueden usar transformaciones no-lineales para encontrar cierta estructura semántica (análisis de documentos). Imagen tomada de citep:Salakhutdinov2009b.
#+attr_html: :width 1200 :align center
[[file:images/20220518-110143_screenshot.png]]

#+REVEAL: split
- Detección de datos atípicos: Máquinas de soporte vectorial

#+DOWNLOADED: screenshot @ 2022-05-18 11:09:10
#+caption: Detección de patrones, imagen tomada de los ejemplos de [[https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py][scikit-learn]]. 
#+attr_html: :width 700 :align center
#+attr_latex: :width .65\linewidth
  [[file:images/20220518-110910_screenshot.png]]


** Observaciones 

Los métodos utilizan distintos ~supuestos~ para poder construirse y necesitan ~validarse~ en la práctica. 

* Análisis de componentes principales

- PCA es capaz de crear una representación de menor dimensión.
- Con PCA se puede ~crear un espacio de atributos~ que podría ser utilizados para tareas supervisadas.

** El modelo

- Podemos pensar en PCA como un mecanismo iterativo para crear atributos.
- El primer atributo que construimos a partir de $X_1, \ldots, X_p$ es 
  \begin{align}
  Z_1 = \phi_{11} X_{1} + \cdots + \phi_{p1} X_p\,,
  \end{align}
  en donde $Z_1$ tiene la mayor varianza posible.
- Por simplicidad pedimos que $\sum_{j} \phi_{j1}^2 = 1$.
- En la literatura se llaman a los coeficientes $\phi_{11}, \ldots, \phi_{p1}$ ~cargas~ o /loadings/.

** Proceso

- Supongamos que tenemos tenemos un conjunto de datos que podemos organizar en $X \in \mathbb{R}^{n\times p}$.
- Supondremos que todas las columnas han sido ~centradas~ (es decir, el promedio
  de cada columna es igual a 0).
- Buscamos 
  \begin{align}
  z_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}\,,
  \end{align}
  para cada $i = 1, \ldots, n$.
- Nota que la ~media~ de las $z_{i1}$ es cero y podemos escribir la ~varianza~ en términos de $\frac1n \sum_{i}^{} z_{i1}^2$.

** Formulación

- Buscamos resolver el problema
  \begin{align}
  \max_{\phi_1 \in \mathbb{R}^p}\frac1n \sum_{i = 1}^{n} \left( \sum_{j = 1}^{p} \phi_{j1} x_{ij} \right)^2\,, \qquad \text{ sujeto a } \qquad \sum_{j = 1}^{p} \phi_{j1}^2 = 1\,.
  \end{align}
- Este problema se puede resolver utilizando la ~descomposición espectral~ de la matriz $X$.
- Definimos $Z_1$ como el primer ~componente principal~.

** Geometría

- El vector de cargas $\phi_1$ define la dirección en el espacio de atributos originales en la que los datos varían más.
- Si proyectamos en esta dirección, entonces recuperamos los /scores/ $z_{11}, \ldots, z_{n1}$.

** Proceso: más componentes

- El segundo componente principal será una combinación lineal de los atributos,
  tal que tenga ~máxima varianza~ y que sea ortogonal a $Z_1$.
- Por lo tanto los /scores/ toman la forma
  \begin{align}
  z_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip}\,,
  \end{align}
  donde el vector $\phi_2$ es el vector de cargas del segundo componente principal.

** Solución 

- La restricción de ortogonalidad sobre los componentes nos permite encontrar
  las direcciones $\phi_1, \phi_2, \ldots$  como los vectores propios por la
  derecha de $X$.
- Las varianzas de los componentes están dados por $\frac1n \times \lambda_i^2$
  donde $\lambda_i$ son los valores propios.
- A lo más, hay $\min(n-1, p)$ componentes principales.


** Ejemplo:

- Tenemos datos del número de arrestos por cada 100,000 habitantes por distintos
  tipos de crímenes: ~Assault~, ~Murder~, and ~Rape~. También tenemos la proporción de
  población que vive en zonas urbanas, ~UrbanPop~.

- El registro es de 50 ciudades en EUA. Por lo tanto $n = 50$ y $p = 4$.

#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-18 12:11:30
#+caption: Imagen tomada de citep:James2021. Gráfico tipo /biplot/ que muestra los /scores/ (observaciones) y los /loadings/ (atributos).
#+attr_html: :width 700 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220518-121130_screenshot.png]]


** Interpretación

- El primer componente principal define la línea en un espacio de $p$ dimensiones que es la más cercana a nuestras $n$ observaciones (bajo distancia Euclideana).
- Esta idea se extiende naturalmente a buscar hiper-planos en más dimensiones.
- Por ejemplo, con los dos componentes recuperamos el plano mas cercano a los datos. 

** Escala en los atributos

#+DOWNLOADED: screenshot @ 2022-05-18 12:18:50
#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 1200 :align center
[[file:images/20220518-121850_screenshot.png]]

** Proporción de varianza explicada

- Nos interesa medir la proporción de varianza de cada componente para entender la importancia o la capacidad de resumen de cada componente principal.
- La ~varianza total~ en el conjunto de datos se define como
  \begin{align}
  \sum_{j = 1}^{p} \mathbb{V}(X_j) \approx \sum_{j = 1}^{p} \frac1n \sum_{i= 1 }^{n} x_{ij}^2\,,
  \end{align}
  la ~varianza explicada~ por cada componente principal la estimamos
  \begin{align}
  \mathbb{V}(Z_m) \approx \frac1n \sum_{i = 1}^{n} z_{im}^2\,.
  \end{align}

#+REVEAL: split
- Se puede probar que
  \begin{align}
  \sum_{j = 1}^{p}\mathbb{V}(X_j) = \sum_{m = 1}^{M} \mathbb{V}(Z_m)\,,
  \end{align}
  con $M = \min(n-1, p)$.
- Por lo tanto podemos calcular la ~proporción de varianza explicada~ como
  \begin{align}
  \frac{\mathbb{V}(Z_m)}{\sum_{j = 1}^{p} \mathbb{V}(X_j)}\,.
  \end{align} 


#+DOWNLOADED: screenshot @ 2022-05-18 12:41:22
#+caption: Imagen tomada de citep:James2021.
#+attr_html: :width 700 :align center
[[file:images/20220518-124122_screenshot.png]]

** ¿Cuántos componentes habrá que utilizar?

/Depende/.




* Segmentación

- Análisis de conglomerados, segmentación, agrupación, ó /clustering/ es una técnica que busca encontrar grupos de
  observaciones. 
- Las observaciones ~dentro~ de los grupos se deben de parecer lo mas posible.
- Las observaciones ~entre~ grupos deben de ser lo mas distintas posible.
- ¿Cómo medimos /similaridad/ o /diferencias/?
  - Con conocimiento de dominio y específico a la aplicación que estamos tratando.

** Conglomerados para segmentación de mercados

- Identificar grupos de personas que sean mas receptivas a un tipo particular de
  anuncios o que sean mas propensas a comprar un producto.


** Métodos de segmentación (ejemplos)

- $K$ /medias/, busca identificar los grupos. 
- /Clustering jerárquico/, no sabemos cuántos grupos necesitamos.
- Modelo de mezclas Gaussianas.



* $K$ medias

Con un conjunto de datos ficticios de $n = 150$ y $p = 2$, obtenemos los
siguientes grupos utilizando valores distintos para $K$.

#+DOWNLOADED: screenshot @ 2022-05-18 16:44:42
#+attr_html: :width 1200 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220518-164442_screenshot.png]]


** ¿Cómo funciona?

Sean $C_1, \ldots, C_K$ los conjuntos de índices a los que pertenece cada instancia.
Estos conjuntos satisfacen:
1. $C_1 \cup C_2 \cup \cdots \cup C_K = \{1, \ldots, n\}$.
2. $C_j \cap C_k = \emptyset$ para toda $j \neq k$.

#+REVEAL: split
- Un buen algoritmo de $K$ medias es aquel que tiene una ~variación dentro de grupos~ pequeña.
- La métrica $\mathsf{WCV}(C_k)$ representa qué tan diferentes son las observaciones dentro del grupo $k$.
- Buscamos resolver:
  \begin{align}
  \min_{C_1, \ldots, C_K} \sum_{k = 1}^{K} \mathsf{WCV}(C_k)\,.
  \end{align}

#+REVEAL: split
- Usualmente usamos distancia Euclideana
  \begin{align}
  \mathsf{WCV}(C_k) = \frac{1}{|C_k|} \sum_{i, j \in C_k}^{} \|x_{i}  - x_{j}\|^2\,,
  \end{align}
  donde $|C_k|$ denota la cuántas observaciones pertenecen al grupo $k$.

  

** Algoritmo

1. Asignar al azar un grupo a cada observación.
2. Repetir hasta que las asignaciones no cambien:
   1. Para cada grupo, encontrar el centro.
   2. Asignar a cada observación el centro mas cercano.
   3. Asignar el grupo al que pertenece.


** Propiedades del algoritmo

- El algoritmo tiene garantía de disminuir la función objetivo en cada iteración.
- Nota que
  \begin{align}
  \frac{1}{|C_k|} \sum_{i, j \in C_k}^{} \|x_i - x_j \|^2 = 2 \sum_{i \in C_k}^{} \|x_i - \bar x_k\|^2\,,
  \end{align}
  donde $\bar x_k$ es el centro del grupo $k$.


#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-18 17:01:09
#+caption: Imagen tomada de citep:James2021.
#+attr_html: :width 700 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220518-170109_screenshot.png]]


#+REVEAL: split
#+DOWNLOADED: screenshot @ 2022-05-18 17:04:08
#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 700 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220518-170408_screenshot.png]]


* Agrupación jerárquica

- No necesitamos determinar el número de grupos que necesitamos.
- El procedimiento es iterativo en donde los grupos se van juntando poco a poco. 

#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 1200 :align center
[[file:images/20220518-171600_screenshot.png]]

#+REVEAL: split

#+DOWNLOADED: screenshot @ 2022-05-18 17:16:58
#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 700 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20220518-171658_screenshot.png]]

#+REVEAL: split

#+DOWNLOADED: screenshot @ 2022-05-18 17:18:54
#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 1200 :align center
[[file:images/20220518-171854_screenshot.png]]


** Puntos a considerar

- Escalas en los atributos.
- Escoger la métrica.
- Escoger el número de /clusters/.
- Escoger los atributos que se usarán.

* Referencias                                                         :latex:

bibliographystyle:abbrvnat
bibliography:references.bib
